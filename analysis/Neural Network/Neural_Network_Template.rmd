---
title: "Support Vector Machine Analysis - Student Dropout Prediction"
subtitle: "Understanding the Pathways to Academic Achievement"
date: "`r Sys.Date()`"
output: # output both html and pdf, need to check if both output aligns in the end
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    code_folding: hide
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,           # Show code by default (can be hidden with code_folding in HTML)
  message = FALSE,       # Hide package loading messages
  warning = FALSE,       # Hide warnings
  fig.width = 10,
  fig.height = 6,
  fig.align = "center"
)

# Load required libraries
library(tidyverse)
library(caret)
library(nnet)

```

# Neural Network Analysis

## Objective
We want our neural network to play the role of an early-warning whether a student is likely to drop out or graduate by learning the messy, non-linear patterns hidden in data.

## Predictive features choosing preprocessed

We focus on three feature blocks available in the preprocessed dataset:

1. **First semesters performance:** 1st- and 2nd-semester grades plus approved credits capture the strongest signals from our ANOVA screening during data preprocessing.
2. **Financial pressure/support:** Tuition fees up to date, debtor status, and scholarship holder tell us whether financial stress the student faces.
3. **Entry profile:** Admission grade and age at enrollment anchor the model with information about students’ starting point.

## Data Loading & Preprocessing

```{r load-data}
# Load data
data <- read.csv("../../data/preprocessed_data.csv", stringsAsFactors = TRUE)

# Keep all three outcome classes so NN addresses the same business question
data$Target <- factor(data$Target, levels = c("Dropout", "Enrolled", "Graduate"))

# Selected features
nn_features <- c(
  "Curricular.units.1st.sem..approved.",
  "Curricular.units.1st.sem..grade.",
  "Curricular.units.2nd.sem..approved.",
  "Curricular.units.2nd.sem..grade.",
  "Tuition.fees.up.to.date",
  "Scholarship.holder",
  "Age.at.enrollment",
  "Admission.grade",
  "Debtor"
)

# Create modeling dataset
nn_model_data <- data %>%
  select(all_of(nn_features), Target) %>%
  na.omit()

# Scaling (important for neural networks)
numeric_cols <- sapply(nn_model_data, is.numeric)
nn_model_data[, numeric_cols] <- scale(nn_model_data[, numeric_cols])

cat("Dataset size:", nrow(nn_model_data), "students\n") # cat() is R function that prints text to console in  single line
cat("Class distribution:\n")
table(nn_model_data$Target) # counts how many records fall into each target class to preview
```

## Train/Test Split

```{r train-test-split}
set.seed(123)
train_idx <- createDataPartition(nn_model_data$Target, p = 0.8, list = FALSE) # not using runif(nrow(GermanCredit)) < 0.8 simply to sample ~80% of rows without worrying about class balance
train_nn <- nn_model_data[train_idx, ]
test_nn <- nn_model_data[-train_idx, ]

cat("Training set:", nrow(train_nn), "samples\n")
cat("Test set:", nrow(test_nn), "samples\n")
```

## Model Training

### Option 1: Lab 1-style `nnet` (single hidden layer)

This mirrors the German Credit lab: one hidden layer, weight decay for regularization, and a compact architecture that works well on tabular socioeconomic data like ours.

```{r train-nnet}
# Train neural network with one hidden layer (Lab 1 style)
# size = number of hidden units, decay = regularization parameter
nn_model <- nnet(
  Target ~ .,
  data = train_nn,
  size = 10,        # 10 hidden units
  decay = 0.01,     # Regularization
  maxit = 500,      # Maximum iterations
  trace = FALSE     # Don't print training progress
)

cat("Model trained successfully!\n")
cat("Hidden units:", nn_model$n[2], "\n")
cat("Weights:", length(nn_model$wts), "\n")
```

### Option 2: Lab 2-style `neuralnet` (deeper architecture)

The Iris lab introduced `neuralnet` for experimenting with multiple hidden layers and visualizing weights. To keep factors compatible we first create dummy variables, then train a deeper network (chunk is off by default—enable once the prep runs).

```{r train-neuralnet, eval=FALSE}
# library(neuralnet)
# library(caret)
#
# # One-hot encode predictors (Lab 2 approach)
# dummies <- dummyVars(Target ~ ., data = train_nn)
# train_nn_matrix <- data.frame(predict(dummies, newdata = train_nn))
# train_nn_matrix$Target <- train_nn$Target
#
# formula_nn <- as.formula(paste("Target ~", paste(setdiff(names(train_nn_matrix), "Target"), collapse = " + ")))
#
# nn_model_alt <- neuralnet(
#   formula_nn,
#   data = train_nn_matrix,
#   hidden = c(10, 5),  # Two hidden layers similar to Lab 2
#   linear.output = FALSE,
#   threshold = 0.01
# )
#
# plot(nn_model_alt)
```

## Model Evaluation

```{r evaluate}
# Predictions on test set
pred_nn_class <- predict(nn_model, newdata = test_nn, type = "class")

# Confusion matrix
conf_matrix <- confusionMatrix(
  as.factor(pred_nn_class),
  test_nn$Target
)

print(conf_matrix)
```

## Performance Metrics Summary

```{r performance-summary}
cat("=== Neural Network Performance (macro-averaged) ===\n\n")
cat("Accuracy:", round(conf_matrix$overall["Accuracy"], 3), "\n")
by_class <- conf_matrix$byClass
cat("Precision:", round(mean(by_class[,"Precision"]), 3), "\n")
cat("Recall:", round(mean(by_class[,"Recall"]), 3), "\n")
cat("F1-Score:", round(mean(by_class[,"F1"]), 3), "\n")
cat("Balanced Accuracy:", round(mean(by_class[,"Balanced Accuracy"]), 3), "\n")
```

## Confusion Matrix Visualization

```{r confusion-matrix-plot}
# Create confusion matrix dataframe
cm_df <- as.data.frame(conf_matrix$table)
names(cm_df) <- c("Prediction", "Reference", "Freq")

# Plot
ggplot(cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), size = 8, color = "white", fontface = "bold") +
  scale_fill_gradient(low = "#3498DB", high = "#E74C3C") +
  labs(title = "Neural Network Confusion Matrix",
       x = "Actual Outcome", y = "Predicted Outcome") +
  theme_minimal() +
  theme(legend.position = "none")
```

## Feature Importance (Optional - requires additional analysis)

```{r feature-importance, eval=FALSE}
# Note: Neural networks don't have straightforward feature importance
# You can use permutation importance or SHAP values
# For simplicity, you can report which features were used

cat("Top features used in model:\n")
cat(paste(nn_features, collapse = ", "), "\n")
```

## Key Findings & Interpretation

### Model Architecture
- **Input Layer:** 9 features (student characteristics and performance)
- **Hidden Layer:** 10 neurons with sigmoid activation
- **Output Layer:** Binary classification (Graduate vs. Dropout)
- **Regularization:** L2 penalty (decay = 0.01) to prevent overfitting

### Performance Highlights
**[Person B: Fill in your actual results]**
- Achieved XX% accuracy on test set
- High recall for graduates (XX%) - few false negatives
- Balanced performance across both classes

### Compared to Linear Models
Neural networks can capture:
- Non-linear relationships between features
- Complex interactions (e.g., age × grades × financial status)
- Threshold effects that linear models miss

### Business Insight
The neural network confirms findings from other models:
- 1st semester performance is critical
- Financial factors (tuition, scholarship) matter
- Non-linear patterns exist (e.g., students with mid-range grades face higher uncertainty)

### Limitations
- **Black box:** Less interpretable than linear models
- **Requires tuning:** Hidden layer size, learning rate, regularization
- **Data hungry:** Performs best with larger datasets

## Comparison with Other Models

| Model | Accuracy | Interpretability | Complexity |
|-------|----------|------------------|------------|
| Logistic Regression (GLM) | ~85% | High | Low |
| GAM | ~87% | Medium | Medium |
| **Neural Network** | **XX%** | **Low** | **High** |
| SVM | TBD | Medium | Medium |

### When to Use Neural Networks
- ✅ High accuracy is priority
- ✅ Large dataset available
- ✅ Complex interactions suspected
- ❌ Interpretability required
- ❌ Limited computational resources

## Conclusions

**Summary of NN findings:**
1. Neural network achieved competitive/superior accuracy compared to linear methods
2. Confirms importance of 1st semester performance and financial factors
3. Captures non-linear patterns that simpler models miss
4. Trade-off: Higher accuracy but less interpretability

**Practical Application:**
- Use NN for **prediction** in early warning system
- Use GLM/GAM for **interpretation** and stakeholder communication
- Ensemble approach: Combine both for robust predictions

---

**Person B: When copying to main report:**
1. Remove the YAML header (keep only the content)
2. Adjust section numbering to match main report
3. Update "XX%" placeholders with your actual results
4. Paste into Section 8 of MASTER_REPORT.rmd
