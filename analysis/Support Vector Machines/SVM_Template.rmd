---
title: "Support Vector Machine Analysis - Student Dropout Prediction"
author: "Person C"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(e1071)       # SVM implementation
library(caret)       # Confusion matrix
library(kernlab)     # Alternative SVM package
```

# Support Vector Machine Analysis

## Objective
Build a robust classifier for student dropout prediction using Support Vector Machines with different kernel functions to handle non-linear decision boundaries.

## Data Loading & Preprocessing

```{r load-data}
# Load data
data <- read.csv("../../data/preprocessed_data.csv", stringsAsFactors = TRUE)

# Filter for Graduate vs Dropout
svm_data <- data %>%
  filter(Target %in% c("Graduate", "Dropout")) %>%
  mutate(Target_binary = as.factor(Target))

# Select features
svm_features <- c(
  "Curricular.units.1st.sem..approved.",
  "Curricular.units.1st.sem..grade.",
  "Curricular.units.2nd.sem..approved.",
  "Curricular.units.2nd.sem..grade.",
  "Tuition.fees.up.to.date",
  "Scholarship.holder",
  "Age.at.enrollment",
  "Admission.grade",
  "Debtor"
)

# Create modeling dataset
svm_model_data <- svm_data %>%
  select(all_of(svm_features), Target_binary) %>%
  na.omit()

# Scale numeric features (important for SVM!)
numeric_cols <- sapply(svm_model_data[, svm_features], is.numeric)
svm_model_data[, svm_features][, numeric_cols] <- scale(svm_model_data[, svm_features][, numeric_cols])

cat("Dataset size:", nrow(svm_model_data), "students\n")
cat("Class distribution:\n")
table(svm_model_data$Target_binary)
```

## Train/Test Split

```{r train-test-split}
set.seed(123)
train_idx <- sample(1:nrow(svm_model_data), 0.8 * nrow(svm_model_data))
train_svm <- svm_model_data[train_idx, ]
test_svm <- svm_model_data[-train_idx, ]

cat("Training set:", nrow(train_svm), "samples\n")
cat("Test set:", nrow(test_svm), "samples\n")
```

## Kernel Comparison

### 1. Linear Kernel

```{r svm-linear}
cat("Training SVM with Linear Kernel...\n")

svm_linear <- svm(
  Target_binary ~ .,
  data = train_svm,
  kernel = "linear",
  cost = 1,
  scale = FALSE  # Already scaled
)

# Predictions
pred_linear <- predict(svm_linear, newdata = test_svm)

# Evaluation
conf_linear <- confusionMatrix(pred_linear, test_svm$Target_binary, positive = "Graduate")

cat("Linear Kernel Accuracy:", round(conf_linear$overall["Accuracy"], 3), "\n")
```

### 2. Polynomial Kernel

```{r svm-polynomial}
cat("Training SVM with Polynomial Kernel...\n")

svm_poly <- svm(
  Target_binary ~ .,
  data = train_svm,
  kernel = "polynomial",
  degree = 3,
  cost = 1,
  scale = FALSE
)

# Predictions
pred_poly <- predict(svm_poly, newdata = test_svm)

# Evaluation
conf_poly <- confusionMatrix(pred_poly, test_svm$Target_binary, positive = "Graduate")

cat("Polynomial Kernel Accuracy:", round(conf_poly$overall["Accuracy"], 3), "\n")
```

### 3. RBF (Radial Basis Function) Kernel

```{r svm-rbf}
cat("Training SVM with RBF Kernel...\n")

svm_rbf <- svm(
  Target_binary ~ .,
  data = train_svm,
  kernel = "radial",
  cost = 1,
  gamma = 0.1,
  scale = FALSE
)

# Predictions
pred_rbf <- predict(svm_rbf, newdata = test_svm)

# Evaluation
conf_rbf <- confusionMatrix(pred_rbf, test_svm$Target_binary, positive = "Graduate")

cat("RBF Kernel Accuracy:", round(conf_rbf$overall["Accuracy"], 3), "\n")
```

## Kernel Performance Comparison

```{r kernel-comparison-table}
kernel_results <- data.frame(
  Kernel = c("Linear", "Polynomial (degree=3)", "RBF (Radial)"),
  Accuracy = c(
    conf_linear$overall["Accuracy"],
    conf_poly$overall["Accuracy"],
    conf_rbf$overall["Accuracy"]
  ),
  Precision = c(
    conf_linear$byClass["Precision"],
    conf_poly$byClass["Precision"],
    conf_rbf$byClass["Precision"]
  ),
  Recall = c(
    conf_linear$byClass["Recall"],
    conf_poly$byClass["Recall"],
    conf_rbf$byClass["Recall"]
  ),
  F1 = c(
    conf_linear$byClass["F1"],
    conf_poly$byClass["F1"],
    conf_rbf$byClass["F1"]
  )
)

knitr::kable(kernel_results, digits = 3, caption = "SVM Kernel Performance Comparison")

# Visualize
ggplot(kernel_results, aes(x = Kernel, y = Accuracy, fill = Kernel)) +
  geom_bar(stat = "identity", alpha = 0.8) +
  geom_text(aes(label = round(Accuracy, 3)), vjust = -0.5) +
  scale_fill_manual(values = c("#3498DB", "#9B59B6", "#E74C3C")) +
  labs(title = "SVM Accuracy by Kernel Type",
       y = "Accuracy") +
  theme_minimal() +
  theme(legend.position = "none")
```

## Hyperparameter Tuning (Best Kernel)

```{r hyperparameter-tuning}
# Tune the best performing kernel (usually RBF)
cat("Tuning hyperparameters for RBF kernel...\n")

# Define parameter grid
tune_grid <- expand.grid(
  cost = c(0.1, 1, 10, 100),
  gamma = c(0.01, 0.1, 1, 10)
)

# Tune using tune.svm
svm_tuned <- tune(
  svm,
  Target_binary ~ .,
  data = train_svm,
  kernel = "radial",
  ranges = list(
    cost = c(0.1, 1, 10, 100),
    gamma = c(0.01, 0.1, 1, 10)
  ),
  scale = FALSE
)

cat("\nBest parameters:\n")
print(svm_tuned$best.parameters)

cat("\nBest cross-validation accuracy:", round(1 - svm_tuned$best.performance, 3), "\n")
```

## Final Model with Best Parameters

```{r final-svm-model}
# Extract best model
best_svm <- svm_tuned$best.model

# Predictions on test set
pred_best <- predict(best_svm, newdata = test_svm)

# Confusion matrix
conf_best <- confusionMatrix(pred_best, test_svm$Target_binary, positive = "Graduate")

print(conf_best)
```

## Performance Summary

```{r performance-summary}
cat("=== Final SVM Performance ===\n\n")
cat("Best Kernel:", svm_tuned$best.model$kernel, "\n")
cat("Best Cost:", svm_tuned$best.parameters$cost, "\n")
cat("Best Gamma:", svm_tuned$best.parameters$gamma, "\n\n")

cat("Test Set Performance:\n")
cat("Accuracy:", round(conf_best$overall["Accuracy"], 3), "\n")
cat("Precision:", round(conf_best$byClass["Precision"], 3), "\n")
cat("Recall:", round(conf_best$byClass["Recall"], 3), "\n")
cat("F1-Score:", round(conf_best$byClass["F1"], 3), "\n")
cat("Balanced Accuracy:", round(conf_best$byClass["Balanced Accuracy"], 3), "\n")

cat("\nSupport Vectors:\n")
cat("Total:", best_svm$tot.nSV, "\n")
cat("Per class:", best_svm$nSV, "\n")
```

## Confusion Matrix Visualization

```{r confusion-matrix-plot}
# Create confusion matrix dataframe
cm_df <- as.data.frame(conf_best$table)
names(cm_df) <- c("Prediction", "Reference", "Freq")

# Plot
ggplot(cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), size = 8, color = "white", fontface = "bold") +
  scale_fill_gradient(low = "#9B59B6", high = "#E74C3C") +
  labs(title = "SVM Confusion Matrix (Best Model)",
       x = "Actual Outcome", y = "Predicted Outcome") +
  theme_minimal() +
  theme(legend.position = "none")
```

## Key Findings & Interpretation

### How SVM Works
Support Vector Machines find the optimal hyperplane that:
1. **Maximizes margin** between classes (Graduate vs. Dropout)
2. **Uses support vectors** (critical boundary cases) to define decision boundary
3. **Kernel trick** allows non-linear boundaries without explicit feature transformation

### Kernel Selection Results
**[Person C: Update with your results]**
- **Linear Kernel:** Assumes data is linearly separable → Good for baseline
- **Polynomial Kernel:** Captures polynomial relationships → Moderate performance
- **RBF Kernel:** Most flexible, handles complex boundaries → **Best performer** (likely)

### Hyperparameter Impact
- **Cost (C):** Controls trade-off between margin width and misclassification
  - High C → Narrow margin, fewer misclassifications (risk: overfitting)
  - Low C → Wide margin, more misclassifications (risk: underfitting)
- **Gamma (γ):** Controls influence of single training example (RBF kernel)
  - High γ → Tight fit to training data (risk: overfitting)
  - Low γ → Smoother decision boundary (risk: underfitting)

### Support Vectors
- **Total SVs:** ~XX% of training data (typical: 20-40%)
- **Interpretation:** These are the "critical cases" on the decision boundary
- **Practical insight:** Students near the boundary need closest monitoring

### Business Insight
SVM reveals:
1. **Clear separation** between graduates and dropouts exists in feature space
2. **Support vectors** represent "at-risk" students who could go either way
3. **Non-linear boundaries** confirm that simple thresholds don't work
4. **Robust to outliers** - SVM only cares about boundary cases

### Advantages of SVM
- ✅ Robust to outliers (only uses support vectors)
- ✅ Works well in high-dimensional space
- ✅ Flexible with kernel tricks
- ✅ Strong theoretical foundation

### Limitations
- ❌ Sensitive to scaling (must normalize features)
- ❌ Hyperparameter tuning required
- ❌ Computationally expensive for large datasets
- ❌ Less interpretable than linear models

## Comparison with Other Models

| Model | Accuracy | Interpretability | Training Time |
|-------|----------|------------------|---------------|
| Logistic Regression | ~85% | High | Fast |
| GAM | ~87% | Medium | Medium |
| Neural Network | ~XX% | Low | Slow |
| **SVM (RBF)** | **~XX%** | **Medium** | **Medium** |

### When to Use SVM
- ✅ Need robust classification
- ✅ Moderate dataset size (thousands, not millions)
- ✅ High-dimensional data
- ✅ Non-linear boundaries suspected
- ❌ Need feature importance rankings
- ❌ Real-time prediction required (can be slow)

## Conclusions

**Summary of SVM findings:**
1. RBF kernel outperformed linear and polynomial kernels → Non-linear patterns exist
2. Optimal hyperparameters: Cost = X, Gamma = Y (from tuning)
3. XX% of training samples are support vectors → These are critical boundary cases
4. Achieved XX% accuracy, competitive with neural networks

**Practical Application:**
- **Prediction:** Use tuned SVM for robust classification
- **Risk Assessment:** Support vectors identify students on the edge
- **Intervention Priority:** Focus resources on students near decision boundary

**Integration with Other Models:**
- SVM confirms findings from GLM/GAM (same top predictors)
- Slightly higher accuracy than linear models
- More interpretable than neural networks (can visualize decision boundary)
- **Recommendation:** Use SVM in ensemble with other models for final predictions

---

**Person C: When copying to main report:**
1. Remove the YAML header
2. Adjust section numbering to match main report (Section 9)
3. Update "XX%" placeholders with your actual results
4. Paste into MASTER_REPORT.rmd replacing the placeholder comment
