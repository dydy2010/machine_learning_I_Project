library(tidyverse)
library(ggplot2)
library(e1071)
library(caret)
library(kernlab)


data <- read.csv("data/data.csv", sep = ";")
summary(data)
nrow(data)
anyNA(data)
colSums(is.na(data))
str(data)

# for me to know which feature better helps classify, a higher anova score indicates me better predictability

cat("\n=== ANOVA F-Statistics for Individual Features ===\n")

numeric_cols <- sapply(data, is.numeric)
features <- names(data)
features <- features[features != "Target"] 

results <- data.frame(Feature=character(), F_value=numeric(), p_value=numeric(), stringsAsFactors=FALSE)

    for(feat in features) {
    formula <- as.formula(paste(feat, "~ Target"))
    anova_result <- aov(formula, data=data)
    summary_anova <- summary(anova_result)
    f_val <- summary_anova[[1]]$`F value`[1]
    p_val <- summary_anova[[1]]$`Pr(>F)`[1]
    results <- rbind(results, data.frame(Feature=feat, F_value=round(f_val, 2), 
                                        p_value=format(p_val, scientific=TRUE)))
    }

    results <- results[order(-results$F_value),]
    print(results, row.names=FALSE)

"""
The results  indicate that grades and the amount of credits approved have the highest F-Values, 
suggesting they are the most significant predictors for predicting the succcess of graduating or not.

In this case, I want to choose both features of marks to plot my model, since they are both continuous variables (according to: str(data)) 

"""

# Prepare the data frame
marks <- data.frame(
  x.1 = as.numeric(data$Curricular.units.2nd.sem..grade.),  # 2nd semester marks
  x.2 = as.numeric(data$Curricular.units.1st.sem..grade.),  # 1st semester marks
  y = as.factor(data$Target)
)

marks <- dat[complete.cases(dat), ]

# Plot 
ggplot(data = marks, aes(x = x.2, y =x.1, color = y)) + 
  geom_point(size = 2) + 
  scale_color_manual(values=c("Dropout" = "#ff2000", 
                               "Enrolled" = "#828282", 
                               "Graduate" = "#2ecc71"))+
  labs(
    x = "1st Semester Marks",
    y = "2nd Semester Marks"
  )


""" 
Now, I will split the data into training and testing sets, and then fit several SVM models to classify the students based on their marks.
"""

set.seed(123)
trainIndex <- createDataPartition(marks$y, p = 0.8, list = FALSE)
train_data <- marks[trainIndex, ]
test_data <- marks[-trainIndex, ]

cat("Training set size:", nrow(train_data), "\n")
cat("Test set size:", nrow(test_data), "\n")

"""
Now, I will fit the SVM model with a linear, a radial and a polynomial kernel.
"""

svm_linear <- svm(y ~ x.1 + x.2, 
                  data = train_data, 
                  kernel = "linear", 
                  cost = 1)

svm_radial <- svm(y ~ x.1 + x.2, 
                  data = train_data, 
                  kernel = "radial", 
                  cost = 10, 
                  gamma = 0.1)

svm_poly <- svm(y ~ x.1 + x.2, 
                data = train_data, 
                kernel = "polynomial", 
                cost = 1, 
                degree = 3)

"""
Predict and compare the models
"""

pred_linear <- predict(svm_linear, test_data)
pred_radial <- predict(svm_radial, test_data)
pred_poly <- predict(svm_poly, test_data)

cat("\n Linear Kernel Result:\n")
cm_linear <- confusionMatrix(pred_linear, test_data)
print(cm_linear)

cat("\n Radial Kernel Result:\n")
cm_radial <- confusionMatrix(pred_radial, test_data)
print(cm_radial)

cat("\n Polynomial Kernel Result:\n")
cm_poly <- confusionMatrix(pred_poly, test_data)
print(cm_poly)


results <- data.frame(
  Model = c("Linear", "Radial", "Polynomial"),
  Accuracy = c(cm_linear$overall['Accuracy'],
               cm_radial$overall['Accuracy'],
               cm_poly$overall['Accuracy'])
)

plot(svm_linear, train_data, x.1 ~ x.2,
     svSymbol = 1, dataSymbol = 16,
     symbolPalette = c("#ff2000", "#828282", "#2ecc71"),
     color.palette = terrain.colors)
title("Linear Kernel SVM")

plot(svm_radial, train_data, x.1 ~ x.2,
     svSymbol = 1, dataSymbol = 16,
     symbolPalette = c("#ff2000", "#828282", "#2ecc71"),
     color.palette = terrain.colors)
title("Radial Kernel SVM")

plot(svm_poly, train_data, x.1 ~ x.2,
     svSymbol = 1, dataSymbol = 16,
     symbolPalette = c("#ff2000", "#828282", "#2ecc71"),
     color.palette = terrain.colors)
title("Polynomial Kernel SVM")